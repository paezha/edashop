---
title: "Session 3. Exploratory data analysis I: Descriptive statistics"
author:
- name: Antonio Paez
- name: Bruno Dias dos Santos
# Enter your name here:
- name: My Name
subject: "Workshop: Exploratory Data Analysis in `R`"

# The next two sections are for your own benefit. In the highlights you can briefly reflect about your learning experience. After completing the session, use this space to write your thoughts: what did you learn working on this session? What was easy? What was challenging? How were you challenged? What did you do that worked? What would you do differently? You can use more than one paragraph but remember to indent the paragraphs. This summary does not need to be very long, try to write it in about 200 words.
highlights: |
    This is my mini-reflection. Paragraphs must be indented.
    
    It can contain multiple paragraphs.
    
# Write the concepts that in your opinion are threshold concepts in this exercise. A threshold concept is a key idea that once you grasp it, it changes your understanding of a topic, phenomenon, subject, method, etc. Write between three and five threshold concepts that apply to your learning experience working on this exercise.
threshold_concepts: 
- threshold concept 1
- threshold concept 2 
- threshold concept 3
- threshold concept 4

# Do not edit below this line unless you know what you are doing
# --------------------------------------------------------------
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  html_document:
    df_print: paged
  pdf_document:
      # The project-template-default.tex file was heavily  adapted from Steven V. Miller's template for academic manuscripts. See:
    # http://svmiller.com/blog/2016/02/svm-r-markdown-manuscript/
    # https://github.com/svmiller/svm-r-markdown-templates/blob/master/svm-latex-ms.tex
    template: "exercise-template-default.tex"
font-family: Times New Roman    
always_allow_html: true
---

> "We never look beyond our assumptions and what's worse, we have given up trying to meet others; we just meet ourselves."   
>
> --― Muriel Barbery

# Session outline

- What is EDA?
- Data summaries revisited
- Appropriate summary statistics by scale of measurement
- Properties of data: central tendency and spread
- Univariate description
- Bivariate description 
- Multivariate description 

# Reminder

Remember that literate programming asks you to do the hard work up front to make your life easier later.

# Preliminaries

Clear the workspace from _all_ objects:
```{r}
rm(list = ls())
```

Load packages. Remember, packages are units of shareable code that augment the functionality of base `R`. For this session, the following package/s is/are used:
```{r}
library(corrr) # Correlations in R
library(dplyr) # A Grammar of Data Manipulation
library(edashop) # A Package for a Workshop on Exploratory Data Analysis
library(janitor) # Simple Tools for Examining and Cleaning Dirty Data
library(kableExtra) # Construct Complex Table with 'kable' and Pipe Syntax
library(skimr) # Compact and Flexible Summaries of Data
```

We will also load the following data frames for this session:
```{r}
data("cntr_sp_basico")
data("cntr_sp_head")
data("cntr_sp_ipvs")
```

These data frames contain information about census tracts in the state of São Paulo. You can check the documentation in the usual way:

```r
?cntr_sp_ipvs
```

In summary, these three tables provide information about the census sectors in the state of São Paulo. Two tables contain selected variables from two datasets from the 2010 Demographic Census: basic (_basico) and head of household (_head). In addition, the IPVS dataset (_ipvs) provides variables from the São Paulo Social Vulnerability Index (IVSP) at the census sector level.

# What is EDA?

Exploratory Data Analysis is the process of learning from the data by concentrating on its intrinsic characteristics and attributes. [John W. Tukey](https://en.wikipedia.org/wiki/John_Tukey), the statistician most responsible for clarifying the distinction between exploratory and confirmatory data analysis, likened exploratory data analysis to detective work. Exploratory data analysis is useful to discover essential evidence regarding the phenomenon of interest, similar to checking fingerprints and alibis that can be used in a trial - the equivalent of confirmatory data analysis, where hypotheses are tested and the evidence is evaluated.

To effectively deploy EDA, it is important to approach the data with as few assumptions as possible. By allowing the data to speak for themselves, EDA aims to: 

1. _Simplify_ descriptions to make them easier to handle with available cognitive power; and
2. Look _below_ previously described surfaces to make the description more effective.

The main tools of EDA are descriptive statistics and visualization techniques. The focus in this session is on descriptive statistics, with an emphasis on _appropriate_ descriptors for different types of data.

Before proceeding, it is worthwhile to briefly think about the things that we are first interested in when we begin working with a data set. What are the most important characteristics of the data that you care about?

-
-
-

# Data summaries revisited

In the previous session we used the function `summary()` from base `R` to obtain quick summaries of data. These summaries already provided some key information about the data, including descriptive statistics. For example, in our table with information about the São Paulo Social Vulnerability Index:
```{r}
summary(cntr_sp_ipvs)
```

We can see that the function `summary()` understands what type of data it is dealing with, and provides summaries that are different for categorical and quantitative variables. 

An alternative to the basic summary is provided by package {[skimr](https://docs.ropensci.org/skimr/)}. This package implements tools to "skim" data, and produces reports that are easier to read because they separate variables by type, provide a larger set of summary statistics that are appropriate to the type of data, and also generate [_sparklines_](https://en.wikipedia.org/wiki/Sparkline). {skimr} is also pipe-friendly. The basic function is `skim()`. It is possible to skim a complete data frame or parts thereby. For example:

<!-- The output of `skim()` does not render well in the pdf unless prepared as shown in the next chunk. This is why this code is not set to be evaluated, and will not be run during knitting -->
```r
library(skimr)
cntr_sp_ipvs |>
  skim(v28)
```

This is read as "pass `cntr_sp_ipvs` to `select()` to retrieve `v28` (average income of the person responsible for the household)  before skimming". Try the code on your console! You will see that the output includes a summary of the data, with a high level description of the inputs: the number of rows, columns, number of columns by type of data, and any grouping variables. 

To render the code in the PDF file, we will use package {[kableExtra](https://haozhu233.github.io/kableExtra/)}, which includes functionality to format tables. Below is the output of skimming our selected variable; from the output we strip the variable that contains the sparklines (`numeric.hist`), which are tricky to render in PDF. This is then passed to functions `kable()` and `kable_styling()`:
```{r}
cntr_sp_ipvs |>
  skim(v28) |>
  select(-numeric.hist) |>
  kable("latex",
        digits = 2,
        booktabs = TRUE) |>
  kable_styling(latex_options="scale_down")
```

The arguments in `kable()` and `kable_styling()` control the appearence of the table in the output document: how to format the rows (booktabs), how many digits to use, whether to scale down a wide table so that it fits the page. The latex option "scale_down" ensures that very wide tables are scaled to fit the page.

Much more information about the possibilities of working with {kableExtra} can be found in the documentation.

Since {skimr} plays well with {dplyr} it is possible to combine it with other data grammar functions to create phrases that include summary statistics. For example, the next chunk of code uses `group_by()` before skimming the table:
```r
cntr_sp_ipvs |>
  select(v28, 
         IPVS) |>
  group_by(IPVS) |>
skim_without_charts()
```

Try this code in your console. The chunk above is read as "take data frame `cntr_sp_ipvs`, select columns `v28` (average income of the person responsible for the household) and `IPVS` (IPVS group), group by `IPVS`, and skim". The descriptive statistics skimmed include the number of missing observations and completeness of the data, the mean, standard deviation, and [_quantiles_](https://en.wikipedia.org/wiki/Quantile), that is, the values that cut the sample at a certain proportion of observations (e.g., "p50" is the value where the sample is split in two equal parts, the bottom 50% and the top 50%).

The "high vulnerability" (Vulnerabilidade alta ) census tracts had an average household income of $R\$ 735$. The richest census tracts had an average income of $R\$3912$, which is higher than the richest census tract in the "Very high vulnerability" (Vulnerabilidade muito alta) type.

Skimming the full table gives the following:
```r
cntr_sp_ipvs |>
  skim()
```

The summaries are separated by type of data: dates are reported separately from factors and from quantitative (numeric) variables. Appropriate summary statistics are calculated for each type of data, with _appropriate_ being a keyword here.

# Appropriate summary statistics by scale of measurement

Wait, what do you mean by "appropriate summary statistics"??

Recall from Session 2 that not all operations are defined for all scales of measurement. For example, variables in the nominal scale could be compared using only boolean operators "==" (exactly equal to) and "!=" (not equal to). No arithmetic operations are defined for ordinal data. And division and multiplication are not appropriate for interval data.

This has implications for the kind of statistics that are appropriate by scale of measurement.

Consider a commonly used summary statistic: the mean of a variable. The mean is defined as follows:
$$
\bar{x} = \frac{x_1 + x_2 + \cdots + x_n}{n} = \frac{1}{n}\sum_{i-1}^n x_i
$$

Is it appropriate to calculate the mean of a categorical variable? What is the meaning of two cars plus one bicycle divided by three?

To understand which summary statistics are appropriate, we must know what various summary statistics aim to represent, and how they are calculated.

# Properties of data

Summary statistics are information reduction techniques. Recall that the objective of EDA is to see the data from different perspectives. Two important properties of data that we often wish to summarize are their central tendency and dispersion. These are discussed next.

## Central tendency

A measure of central tendency is a summary of a distribution of values that gives a "typical" value, or the one most frequently observed. Conceptually, this is similar to organizing all data values and finding the location of the _center of mass_ of the distribution. To illustrate the concept of center of mass consider the following sequence of quantitative values:
```{r}
x <- c(20, 30, 32, 34, 41, 41, 45, 46, 48, 51, 53, 54, 54, 56, 57, 58, 58, 59, 
  60, 61, 64, 65, 65, 69, 71, 74, 77, 79, 88, 94)
```

The same sequence of values is shown below in the style of a [stem-and-leaf](https://en.wikipedia.org/wiki/Stem-and-leaf_display) table:

stem    | leaf
--------|--------
2       |0
3       |024
4       |11568
5       |134467889
6       |014559
7       |1479
8       |8
9       |4

Where is the distribution "heavier"? Thereabouts will be its center of mass. There are various measures of central tendency, three of which are discussed next.

In the case of nominal variables, the categories do not have a meaningful order, and yet the center of mass is always the same. Consider for instance: 

### Mode

The mode of a distribution is the most frequent value found in a distribution. Since it only involves counting the instances of each values, it is appropriate for nominal and ordinal variables. We can find the mode by tabulating the values. Let us do so for the variable `IPVS` (factor) in data frame `cntr_sp_ipvs`. Here we introduce function `tabyl()` from package {[janitor](http://sfirke.github.io/janitor/)} for pipe-friendly tabulations:
```{r}
cntr_sp_ipvs |> 
  tabyl(AGSN)
```

We see that the mode of the distribution is "Não especial" (Non-subnormal), the most frequent value of the variable in this distribution. (Notice that by default the values of the factor are sorted alphabetically; this can be changed by redefining the factor and changing the order of the levels).

Next, let us try variable `IPVS` (ordered factor):
```{r}
cntr_sp_ipvs |> 
  tabyl(IPVS)
```

We see that the mode of this distribution is "Vulnerabilidade muito baixa" (extremely low vulnerability). Since ordinal variables have by definition a natural order, the shape of their distribution can be conveniently presented in the style of a stem-and-leaf table, with each "I" representing a thousand instances of the value:

stem                        | leaf
----------------------------|--------
Não classificado            |IIIII I
Baixíssima vulnerabilidade  |IIIII 
Vulnerabilidade muito baixa |IIIII IIIII IIIII IIIII IIIII
Vulnerabilidade baixa       |IIIII IIIII 
Vulnerabilidade média       |IIIII IIIII I
Vulnerabilidade alta        |IIIII I
Vulnerabilidade muito alta  |II


### Median

The median is the quantile that splits a quantitative variables in two parts of equal size, the bottom 50% and the top 50% of values.  

Check again the stem-and-leaf table of our sample quantitative variable.

stem    | leaf
--------|--------
2       |0
3       |024
4       |11568
5       |134467889
6       |014559
7       |1479
8       |8
9       |4

There are $n=30$ observations in this vector. Which value splits the distribution in half? The median of quantitative variables is reported both by `summary()` and `skim()`

### Mean

The mean is probably the best known measure of central tendency, and it is defined as the sum of the values divided by the number of observations. Since it involves arithmetic operations it is not appropriate for categorical variables. The mean of quantitative variables is reported by `summary()` and `skim()`.

## Spread

Another important property of a distribution of values is how wide or compact it is. Compare the two steam-and-leaf tables below.

stem    | leaf
--------|--------
2       |0
3       |024
4       |11568
5       |134467889
6       |014559
7       |1479
8       |8
9       |4

stem    | leaf
--------|--------
1       |48
2       |08
3       |024
4       |1156
5       |13789
6       |01459
7       |149
8       |468
9       |45
10      |7

The first stem-and-leaf table is more "compact": the tails of the distribution are closer together and the center of mass is "heavier", compared to the second table, that has a wider spread.

### Minimum and maximum

The minimum and maximum values give an idea of how spread the distribution is. In the first of the preceding tables the minimum is $20$ and the maximum is $94$. In the second table, the minimum is $14$ and the maximum is $107$. The _range_ is the difference between the maximum and the minimum:
```{r}
94 - 20
107 - 14
```

The second distribution is more spread.

### Inter-quartile range

The inter-quartile range is similar to the range, but instead of being calculated using the minimum and maximum values of the distribution, it uses the third and first quartiles. [Quartiles](https://en.wikipedia.org/wiki/Quartile) are a form of quantile that divides a sequence of values in four equal parts, so the second quantile represents the value that separates the lowest 25% of the sample from the remaining 75%, and the third quantile is the value that splits the highest 25% of the sample from the lowest 75%.

If we skim the data, we see that the quartiles are reported ("p25" is the first quartile and "p75" is the third). The inter-quartile range can be calculated using those values.
```r
cntr_sp_ipvs |>
  skim(v28)
```


The difference between the third and first quartile is:
```{r}
1742 - 830
```

We can _pull_ the variable from the data frame, and use function `IQR()` to directly calculate the inter-quartile range. From the skim of the variable we know that there are some missing (NA) records that need to be removed, hence `na.rm = TRUE`:
<!--- Function `pull()` from {dplyr} is the verb to extract one column from a data frame as a vector --->
```{r}
cntr_sp_ipvs |>
  pull(v28) |>
  IQR(na.rm = TRUE)
```

The inter-quartile range involves an arithmetic operation, which is why it is not an appropriate statistic for categorical variables.

### Variance and standard deviation

The variance is another widely used measure of the spread of a distribution. It is defined as:
$$
\sigma^2 = \frac{1}{n}\sum_{i=1}^n(x_i - \bar{x})^2
$$

In this formula, $\bar{x}$ is the mean of $x$ and $n$ is the number of observations in the sample. Accordingly, $x_i-\bar{x}$ is the deviation of $x_i$ from the mean of $x$. If we rewrite this as follows:
$$
z_i = (x_i - \bar{x})^2
$$

It is easy to see that the variance is actually the mean of the square of the deviations from the mean:
$$
\sigma^2 = \frac{1}{n}\sum_{i=1}^nz_i
$$

The standard deviation is simply the square root of the variance and returns the variance to the same units as the original variable. The standard deviation is reported by `skim()` as `sd`, and can also be calculated with function `sd()` (remember to remove the missing values):
```{r}
cntr_sp_ipvs |>
  pull(v28) |>
  sd(na.rm = TRUE)
```

We see that the typical deviation from the mean of `v28` (average income of the person responsible for the household) was about $1715.2$ R$.

# Univariate description

Summary statistics of central tendency and spread refer to a single variable and are appropriately called univariate descriptors. These descriptors are very important, and we neglect exploring them at our own peril. They often tell us important aspects of the data, including how complete a data set is, how much variation is there, whether there are atypical or unusual values.

As an example, let us calculate the mean, standard deviation, and maximum of `v28` (average income of the person responsible for the household):
```{r}
mean_v28 <- cntr_sp_ipvs |> 
  pull(v28) |> 
  mean(na.rm = TRUE)

sd_v28 <- cntr_sp_ipvs |> 
  pull(v28) |> 
  sd(na.rm = TRUE)

max_v28 <- cntr_sp_ipvs |> 
  pull(v28) |> 
  max(na.rm = TRUE)
```

The maximum average income of the person responsible for the household in the data set was R\$ $73,312.50$. Just how common or unusual is this value? That depends on how close (or far away) from the mean of the distribution this is, as well as on the spread of the distribution. The deviation from the mean is:
```{r}
max_v28 - mean_v28
```

That is, approximately R\$ $71,664$. But the typical deviation from the mean in the sample was only about R\$ $1,715.20$! Now, calculating:
```{r}
(max_v28 - mean_v28)/sd_v28
```

This tells us that the census tract with the highest average income receives over forty-one times more than the average income of the census tracts. This observation is indeed quite unusual. How unusual was the census tract with the lowest average income? Let us retrieve the minimum duration:
```{r}
min_v28 <- cntr_sp_ipvs |> 
  pull(v28) |> 
  min(na.rm = TRUE)
```

That is, R\$ $0$. Again, the typical deviation from the mean in the sample was R\$ $1,715.20$! So, calculating:
```{r}
(min_v28 - mean_v28)/sd_v28
```

The census tract with the lowest average income is much closer to the mean, and approximately one standard deviation below the mean.


Univariate description is a powerful way to get to know our data before doing any more sophisticated explorations or analysis.

# Bivariate description

Moving on from univariate description, understanding how two variables relate to one another is another key aspect of EDA. 

## Categorical variables: cross-tabulations

Univariate description of a categorical variable involves tabulating the number of instances of each response. This can be expanded to simultaneously tabulating two categorical variables. Function `tabyl()` can be used, as in the following example:
```{r}
cntr_sp_ipvs |>
  tabyl(AGSN, IPVS) 
```

What do we learn from this table? The output of the `tabyl()` function can be _adorned_, which can improve the readability of the table. The following table gives the total sum of the columns as a row at the bottom of the table:
```{r}
cntr_sp_ipvs |>
  tabyl(AGSN, IPVS) |>
  adorn_totals(where = "row")
```

Or the total sums of the rows as a column at the right of the table:
```{r}
cntr_sp_ipvs |>
  tabyl(AGSN, IPVS) |>
  adorn_totals(where = "col")
```

And the values can be displayed as proportions:
```{r}
cntr_sp_ipvs |>
  tabyl(AGSN, IPVS) |>
  adorn_totals(where = "col") |>
  adorn_percentages(denominator = "col") |>
  adorn_pct_formatting()
```

If our data set has missing observations (NAs), the table can be displayed without the missing values, for example:
```{r}
cntr_sp_ipvs |>
  tabyl(IPVS, AGSN,
        show_na = FALSE) |>
  adorn_totals(where = "both")
```

There are in total $n = 66096$ valid observations when we consider variables `IPVS` and `AGSN` simultaneously. The proportions (by column) are as follows:
```{r}
cntr_sp_ipvs |>
  tabyl(IPVS, AGSN,
        show_na = FALSE) |>
  adorn_totals(where = "row") |>
  adorn_percentages(denominator = "row") |>
  adorn_pct_formatting()
```

What else do we learn from this table?

_If_ the category of the census tracts did not vary by the type of index category, we would expect the percentages in the bottom row to be roughly the same, since every category of census tract would have a uniform chance of being in any state. We can calculate the values in the table _if_ this was true. This is done by multiplying the row total by the column total for each `IPVS` and `AGSN` combination, and dividing by the size of the sample:
$$
E_{ij} = \frac{1}{n} \sum_ix_i \sum_jx_j
$$
For example, the expected value for "Baixíssima vulnerabilidade" and "Subnormal" is:
```{r}
(4980 * 4119)/66096
```

The observed value, on the other hand, is $0$. Therefore, we see that census tracts with very low vulnerability belong to the subnormal class less often than if the precariousness of the census tracts were random. It is possible to summarize the differences between the observed and expected values in a cross-tabulation:
$$
\chi^2=\sum_i\sum_j\frac{(O_{ij} - E_{ij})^2}{E_{ij}}
$$

If the observed and expected values are identical in every case, the value of $\chi^2$ would be zero. Contrariwise, $\chi^2$ will tend to grown as the differences between observed and expected counts grow. This would suggest that the observed counts are unlikely to follow a random pattern.

The output of function `tabyl()` can be passed to function `chisq.test()` which computes $\chi^2$ and produces also a $p$-value to aid in the decision whether the distribution follows a random pattern or not. A small $p$-value would indicate a small probability of the distribution being random:
```{r}
chi2 <- cntr_sp_ipvs |>
  tabyl(IPVS, AGSN,
        show_na = FALSE)|>
  chisq.test(tabyl_results = TRUE)

chi2
```

As a by-product, we also get the table with expected values:
```{r}
chi2$expected |>
  adorn_rounding(digits = 2)
```

Which can be compared to the observed counts:
```{r}
chi2$observed
```

We see that the distribution is dominated by census tracts of class "Vulnerabilidade muito baixa", which tend to be close to the expected values.

## Quantitative variables: correlation

The mean and standard deviation are key univariate descriptors of quantitative variables. When we are interested in the relationship between two quantitative variables we use a related concept, the _covariance_. The covariance is the mean of the product of the deviations from the mean of two variables:
$$
C(x,y) = \frac{1}{n}\sum_i(x_i - \bar{x})(y_i - \bar{y})
$$
In the formula above, $\bar{x}$ and $\bar{y}$ are the means of the two variables. The differences from the mean can be positive, negative, or zero. When both deviations are positive, the product is positive, thus increasing the covariance. Likewise when the two deviations are negative. But when one deviation is positive and the other negative, the product is negative, which subtracts from the covariance. When differences are like-like (positive-positive or negative-negative) the covariance will tend to be a large positive number. The contrary happens when the differences are opposed.

The covariance can be normalized by the standard deviations of the variables to give the correlation coefficient:
$$
r(x, y) = \frac{C(x, y)}{\sigma_x\cdot\sigma_y}
$$
This quantity has the census tract of being bound between [-1, 1], and a value of zero indicates that the two variables do not _covary_. Correlations can be calculated in a pipe-friendly way with function `correlate()` from package {[corrr](https://corrr.tidymodels.org/)}:
```{r}
cntr_sp_ipvs |>
  select(v28, v19) |>
  correlate(method = "pearson",
            use = "pairwise.complete.obs")
```

What does the correlation between these two variables indicate?

# Multivariate description

Multivariate description extends ideas similar to those discussed above but for more than two variables.

## Categorical variables: multiple cross-tabulations

It is possible to cross-tabulate more than two variables, however in practice this is seldom done for more than three because the results quickly become difficult to read and interpret. Since the `cntr_sp_ipvs` has only two categorical variables, we'll join this data set with the `cntr_sp_basico`: 

```{r}
cntr_sp_ipvs <- cntr_sp_ipvs |>
  left_join(cntr_sp_basico,
            by = c("COD_SETOR" = "code_tract"))
```

Function `tabyl()` implements cross-tabulations of up to three variables:
```{r}
cntr_sp_ipvs |>
  tabyl(AGSN, situacao, IPVS) 
```

The $chi^2$ summary of association for a cross-tabulation seen above no longer works for tables in higher dimensions (i.e., when the number of variables cross-tabulated is greater than two). For an example of alternative approaches to describe tables with more than two variables, see the application of Cochran-Mantel-Haenszel $\chi^2$ statistic in Mella-Lira and Paez ([2021](https://doi.org/10.1016/j.jth.2021.101015)).

## Quantitative variables: correlation matrices

Correlations can be explored among multiple variables. The example below shows how to select the quantitative variables from a data frame and obtain a correlation matrix. The matrix is symmetric, so here we `shave()` the output of correlate to show each pair-wise correlation only once:
```{r}
cntr_sp_ipvs |>
  select(where(is.numeric)) |>
  correlate(method = "pearson",
            use = "pairwise.complete.obs") |> 
  shave()
```

Note that the `method` is `pearson`. It is possible to select two other correlation methods that work on ranked data instead of the quantities. Ranking the values is a more robust way of calculating the association between two variables for reasons that will become clear next session when we discuss visualization approaches for EDA.

# Practice

1. Join tables `cntr_sp_ipvs`, `cntr_sp_basico`, and `cntr_sp_head`.

2. Imagine that you are interested in the average age of head of household. Describe and discuss this variable.

3. How does the average age of head of household relate to other variables in the data set? Select 4 variables and discuss.

4. Propose some hypotheses about age of head of household based on your exploration of the data set. How would you propose to investigate these hypotheses?

5. Imagine now that you are interested in the vulnerability group of the census tracts (check `?cntr_sp_ipvs`). Repeat questions 3 and 4 but for this variable.

6. What can you say so far about the relationship between age of head of household and the categorical variables in the data set? Or between vulnerability group  and the quantitative variables in the data set? Propose an approach to explore a combination of categorical and quantitative variables.


